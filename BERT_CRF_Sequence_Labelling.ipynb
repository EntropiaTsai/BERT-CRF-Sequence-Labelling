{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o62ZE7557q3h"
      },
      "source": [
        "# BERT-CRF on sequence labelling\n",
        "\n",
        "Tutorial of KnULP lab meetning, National Chengchi University\n",
        "\n",
        "*Chang-Yu Tsai, 2025.04.25*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgv0XAba8u2k"
      },
      "source": [
        "- In this week, we will try:\n",
        "  - to obtain embeddings from BERT, instead of conducting any tasks with BERT\n",
        "  - to feed the embeddings to CRF to conduct sequence labelling.\n",
        "- We will compare the performance on two approaches to obtaining embeddings:\n",
        "\n",
        "  1. Using the pre-trained BERT of Hugging Face to obtain embeddings.\n",
        "  2. Fine-tuning the pre-trained BERT of Hugging Face first, and using the fine-tuned embeddings.\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZLsbeL47IKB"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- installing `pytorhc-crf`"
      ],
      "metadata": {
        "id": "G7R-LXCMjP-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uopCOnmQ_LIe"
      },
      "outputs": [],
      "source": [
        "pip install pytorch-crf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDM0eyYjWh8W"
      },
      "source": [
        "- importing required packages\n",
        "\n",
        "```\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import json\n",
        "\n",
        "from torchcrf import CRF\n",
        "\n",
        "import os\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccSIYTYA8uEZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import json\n",
        "\n",
        "from torchcrf import CRF\n",
        "\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFfZjYmSC7Qy"
      },
      "source": [
        "- download the dataset from `Github`\n",
        "\n",
        "The dataset is collected from [Chinese HealthNER Corpus](https://github.com/NYCU-NLP/Chinese-HealthNER-Corpus). Chinese Healthcare Named Entity Recognition (HealthNER) Corpus is collected and annotated by [NYCU NLP Lab](https://ainlp.tw/).\n",
        "\n",
        "```\n",
        "!wget https://raw.githubusercontent.com/NYCU-NLP/Chinese-HealthNER-Corpus/a5eaca54376267cee7a015eb870f7f302517d813/train.zip\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni8C7zAnMfnV"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/NYCU-NLP/Chinese-HealthNER-Corpus/a5eaca54376267cee7a015eb870f7f302517d813/train.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- unzipping the file\n",
        "\n",
        "```\n",
        "!unzip train.zip\n",
        "```"
      ],
      "metadata": {
        "id": "Uhm08r1iBDf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHLkHqoK50az"
      },
      "outputs": [],
      "source": [
        "!unzip train.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGeVMf9eXRMR"
      },
      "source": [
        "- reading the file\n",
        "\n",
        "```\n",
        "data = []\n",
        "with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "# You can choose to work on smaller or bigger data according to your device.\n",
        "data = data[:2000]\n",
        "\n",
        "print(\"Row number:\", len(data))\n",
        "print(\"The first row:\\n\", data[0])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_VUlveCC-U9"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "# You can choose to work on smaller or bigger data according to your device.\n",
        "data = data[:2000]\n",
        "\n",
        "print(\"Row number:\", len(data))\n",
        "print(\"The first row:\\n\", data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjoAyYL2ZgRd"
      },
      "source": [
        "- inspecting the key of the dataset\n",
        "\n",
        "This `json` includes different keys pairing to different values. We will only work with some of them.\n",
        "```\n",
        "data[0].keys()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fdn5tETVEH7"
      },
      "outputs": [],
      "source": [
        "data[0].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### obtainnig texts\n",
        "\n"
      ],
      "metadata": {
        "id": "Ui7RX-Ppp57S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVnW5idiZ0TF"
      },
      "source": [
        "- extracting the data we are going to work on NER\n",
        "\n",
        "We are working on the character-level sequence labelling task. Therefore, we first obtain the characters and the character-level labels from the dataset. As for `sentences`, you can optionally obtain it if you would like to check the original sentence.\n",
        "\n",
        "```\n",
        "sentences=[]\n",
        "characters=[]\n",
        "character_labels=[]\n",
        "for i in range(len(data)):\n",
        "  sentences.append(data[i]['sentence'])\n",
        "  characters.append(data[i]['character'])\n",
        "  character_labels.append(data[i]['character_label'])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1ZbuhlsFuvX"
      },
      "outputs": [],
      "source": [
        "sentences=[]\n",
        "characters=[]\n",
        "character_labels=[]\n",
        "for i in range(len(data)):\n",
        "  sentences.append(data[i]['sentence'])\n",
        "  characters.append(data[i]['character'])\n",
        "  character_labels.append(data[i]['character_label'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jB2nHkEZ8X_"
      },
      "source": [
        "- calculating the max length of the sentence for the padding later\n",
        "\n",
        "```\n",
        "length_list=[]\n",
        "for char_list in characters:\n",
        "  length=len(char_list)\n",
        "  length_list.append(length)\n",
        "max_len=max(length_list)\n",
        "print('The maximum sentence length:', max_len)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLcOtYzCtmUv"
      },
      "outputs": [],
      "source": [
        "length_list=[]\n",
        "for char_list in characters:\n",
        "  length=len(char_list)\n",
        "  length_list.append(length)\n",
        "max_len=max(length_list)\n",
        "print('The maximum sentence length:', max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XEg7wG5Zl5_"
      },
      "source": [
        "- taking a look at the dataset\n",
        "  - `sentence`: the string of the text\n",
        "  - `character`: the list of characters of the text\n",
        "  - `character_label`: the list of character-level label of the text, based on the BIO schema\n",
        "\n",
        "```\n",
        "data[0]['sentence']\n",
        "# data[0]['character']\n",
        "# data[0]['character_label']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLRoFeoFFRwA"
      },
      "outputs": [],
      "source": [
        "data[0]['sentence']\n",
        "# data[0]['character']\n",
        "# data[0]['character_label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u9lQDgX7NUM"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d_ygcoGeOHr"
      },
      "source": [
        "### text encoding\n",
        "It is important to conduct **tokenising** and **aligning** for text encoding before we work on BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q0KIhHVeOH0"
      },
      "source": [
        "##### tokenising\n",
        "\n",
        "`tokenizer` in HuggingFace Transformers is a built-in utility that handles not only tokenisation, but also:\n",
        "- Automatic padding: With `padding='max_length'`, it pads all sequences to the maximum length of the dataset.\n",
        "\n",
        "- Tensor output: With `return_tensors=\"pt\"`, it directly returns PyTorch tensors (no need to convert manually)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLuHkSGeeOH2"
      },
      "source": [
        "- defining the function to encode the texts\n",
        "\n",
        "  **ðŸ‘“ Note that we also output `alignment_ids_list`, which is used to perform the following alignment between tokens and labels.**\n",
        "  \n",
        "  - We rely on the `.word_ids()` method to determine the alignment between subword tokens and their corresponding original characters.\n",
        "    - `word_ids` assigns an integer index for each subword, indicating which original token (i.e., character) it comes from.\n",
        "    - Special tokens such as `[CLS]`, `[SEP]`, and `[PAD]` will be assigned `None`, since they do not correspond to any original token. These should be ignored during label alignment.\n",
        "\n",
        "\n",
        "```\n",
        "def character_encode(characters, character_labels, label2id, tokenizer_path, local_files_only=None):\n",
        "    \"\"\"\n",
        "    Encode character-level inputs and prepare alignment information for NER tasks.\n",
        "\n",
        "    Args:\n",
        "        characters (List[List[str]]): List of List of characters\n",
        "        character_labels (List[List[str]]): List of corresponding labels\n",
        "        label2id (Dict[str, int]): Mapping from label string to ID\n",
        "        tokenizer_path (str): Path to local tokenizer or Hugging Face model ID\n",
        "        local_files_only (bool, optional): Whether to only load from local files.\n",
        "                                           If None, automatically detect.\n",
        "\n",
        "    Returns:\n",
        "        input_ids_tensor, attention_mask_tensor, alignment_ids_list\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Load tokenizer ---\n",
        "    if local_files_only is None:\n",
        "        # If user didn't specify, automatically check if path exists\n",
        "        local_files_only = os.path.exists(tokenizer_path)\n",
        "\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path, local_files_only=local_files_only)\n",
        "\n",
        "    # --- Tokenize inputs ---\n",
        "    encodings = tokenizer(\n",
        "        characters,                    # List of List of characters\n",
        "        is_split_into_words=True,       # The input is already split so it won't be split into subwords.\n",
        "        padding='max_length',           # Padding based on maximum length specified in `max_length`\n",
        "        max_length=max_len,             # Specifying the maximum length\n",
        "        truncation=True,                # Truncating the input if its length is longer than `max_len`\n",
        "        return_tensors=\"pt\"             # Return the torch tensors\n",
        "    )\n",
        "\n",
        "    input_ids_tensor = encodings['input_ids']\n",
        "    attention_mask_tensor = encodings['attention_mask']\n",
        "\n",
        "    # --- Preparation for alignment ---\n",
        "    alignment_ids_list = []\n",
        "    for i in range(len(characters)):\n",
        "        alignment_ids = encodings.word_ids(batch_index=i)   # Obtaining the alignment IDs through `word_ids`\n",
        "        alignment_ids_list.append(alignment_ids)\n",
        "\n",
        "    return input_ids_tensor, attention_mask_tensor, alignment_ids_list\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QoMGM0peOH3"
      },
      "outputs": [],
      "source": [
        "def character_encode(characters, character_labels, label2id, tokenizer_path, local_files_only=None):\n",
        "    \"\"\"\n",
        "    Encode character-level inputs and prepare alignment information for NER tasks.\n",
        "\n",
        "    Args:\n",
        "        characters (List[List[str]]): List of List of characters\n",
        "        character_labels (List[List[str]]): List of corresponding labels\n",
        "        label2id (Dict[str, int]): Mapping from label string to ID\n",
        "        tokenizer_path (str): Path to local tokenizer or Hugging Face model ID\n",
        "        local_files_only (bool, optional): Whether to only load from local files.\n",
        "                                           If None, automatically detect.\n",
        "\n",
        "    Returns:\n",
        "        input_ids_tensor, attention_mask_tensor, alignment_ids_list\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Load tokenizer ---\n",
        "    if local_files_only is None:\n",
        "        # If user didn't specify, automatically check if path exists\n",
        "        local_files_only = os.path.exists(tokenizer_path)\n",
        "\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path, local_files_only=local_files_only)\n",
        "\n",
        "    # --- Tokenize inputs ---\n",
        "    encodings = tokenizer(\n",
        "        characters,                    # List of List of characters\n",
        "        is_split_into_words=True,       # The input is already split so it won't be split into subwords.\n",
        "        padding='max_length',           # Padding based on maximum length specified in `max_length`\n",
        "        max_length=max_len,             # Specifying the maximum length\n",
        "        truncation=True,                # Truncating the input if its length is longer than `max_len`\n",
        "        return_tensors=\"pt\"             # Return the torch tensors\n",
        "    )\n",
        "\n",
        "    input_ids_tensor = encodings['input_ids']\n",
        "    attention_mask_tensor = encodings['attention_mask']\n",
        "\n",
        "    # --- Preparation for alignment ---\n",
        "    alignment_ids_list = []\n",
        "    for i in range(len(characters)):\n",
        "        alignment_ids = encodings.word_ids(batch_index=i)   # Obtaining the alignment IDs through `word_ids`\n",
        "        alignment_ids_list.append(alignment_ids)\n",
        "\n",
        "    return input_ids_tensor, attention_mask_tensor, alignment_ids_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bYOMQNir9qu"
      },
      "source": [
        "#### aligning\n",
        "It is very important to align tokens with labels after tokenising with BERT, since BERT performs tokenisation at the subword level.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLVesjNqCEn8"
      },
      "source": [
        "- defining the function to perform the alignment\n",
        "\n",
        "```\n",
        "def align_labels(alignment_ids_list, character_labels, label2id):\n",
        "    label_ids_list = []\n",
        "\n",
        "    for alignment_ids, label_seq in zip(alignment_ids_list, character_labels):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "\n",
        "        for word_idx in alignment_ids:\n",
        "            if word_idx is None:                  # if this token is CLS/SEP/PAD\n",
        "                label_ids.append(-100)            # assigning the label ID -100\n",
        "\n",
        "            elif word_idx != previous_word_idx:   # if this token is not the same as the previous one\n",
        "                label_str = label_seq[word_idx]   # obtaining the corresponding label\n",
        "                label_id = label2id[label_str]    # obtaining the corresponding ID\n",
        "                label_ids.append(label_id)        # saving the ID\n",
        "\n",
        "            else:                                 # if it is a continuation subword of the previous one\n",
        "                label_ids.append(-100)            # assigning the label ID -100\n",
        "\n",
        "            previous_word_idx = word_idx          # saving the current ID for the next loop\n",
        "\n",
        "        label_ids_list.append(label_ids)          # saving the label of the same sentence into one list\n",
        "\n",
        "    label_tensor = torch.tensor(label_ids_list)   # converting the label into `torch.tensor`\n",
        "    return label_tensor\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJdVbyE1bfzU"
      },
      "outputs": [],
      "source": [
        "def align_labels(alignment_ids_list, character_labels, label2id):\n",
        "    label_ids_list = []\n",
        "\n",
        "    for alignment_ids, label_seq in zip(alignment_ids_list, character_labels):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "\n",
        "        for word_idx in alignment_ids:\n",
        "            if word_idx is None:                  # if this token is CLS/SEP/PAD\n",
        "                label_ids.append(-100)            # assigning the label ID -100\n",
        "\n",
        "            elif word_idx != previous_word_idx:   # if this token is not the same as the previous one\n",
        "                label_str = label_seq[word_idx]   # obtaining the corresponding label\n",
        "                label_id = label2id[label_str]    # obtaining the corresponding ID\n",
        "                label_ids.append(label_id)        # saving the ID\n",
        "\n",
        "            else:                                 # if it is a continuation subword of the previous one\n",
        "                label_ids.append(-100)            # assigning the label ID -100\n",
        "\n",
        "            previous_word_idx = word_idx          # saving the current ID for the next loop\n",
        "\n",
        "        label_ids_list.append(label_ids)          # saving the label of the same sentence into one list\n",
        "\n",
        "    label_tensor = torch.tensor(label_ids_list)   # converting the label into `torch.tensor`\n",
        "    return label_tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFkd7kJlGgwq"
      },
      "source": [
        "### obtaining the IDs of labels\n",
        "We will later encode labels as tensors, and at first, we need to creat the mapping list of IDs and labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj9ZNpvIbCQv"
      },
      "source": [
        "- defining a function to create mapping lists\n",
        "\n",
        "```\n",
        "def label_and_id(label_seqs):\n",
        "    all_labels = set()\n",
        "    for seq in label_seqs:\n",
        "        for label in seq:\n",
        "            all_labels.add(label)\n",
        "\n",
        "    # sorting\n",
        "    label_list = sorted(all_labels)\n",
        "\n",
        "    # building the mapping lists\n",
        "    label2id = {}\n",
        "    for i, label in enumerate(label_list):\n",
        "        label2id[label] = i\n",
        "    id2label = {}\n",
        "    for label, i in label2id.items():\n",
        "        id2label[i] = label\n",
        "\n",
        "    return label2id, id2label\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRk5yC70tRaP"
      },
      "outputs": [],
      "source": [
        "def label_and_id(label_seqs):\n",
        "    all_labels = set()\n",
        "    for seq in label_seqs:\n",
        "        for label in seq:\n",
        "            all_labels.add(label)\n",
        "\n",
        "    # sorting\n",
        "    label_list = sorted(all_labels)\n",
        "\n",
        "    # building the mapping lists\n",
        "    label2id = {}\n",
        "    for i, label in enumerate(label_list):\n",
        "        label2id[label] = i\n",
        "    id2label = {}\n",
        "    for label, i in label2id.items():\n",
        "        id2label[i] = label\n",
        "\n",
        "    return label2id, id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wS-eNPzcTSo"
      },
      "source": [
        "- creating the mapping lists\n",
        "\n",
        "```\n",
        "label2id, id2label=label_and_id(character_labels)\n",
        "print(label2id)\n",
        "print(id2label)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KyJMcLEApa0"
      },
      "outputs": [],
      "source": [
        "label2id, id2label=label_and_id(character_labels)\n",
        "print(label2id)\n",
        "print(id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1dCDfeqwMYQ"
      },
      "source": [
        "### data splitting\n",
        "Before encoding the texts, we split the data into the trainging set (70\\%), the dev set(10\\%), and the test set (20\\%).\n",
        "\n",
        "```\n",
        "# Step 1: 70% for the training set and 30% for the remaining data\n",
        "train_chars, temp_chars, train_labels, temp_labels = train_test_split(\n",
        "    characters, character_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: in the remaining data, 10% for the dev set and 20% for the test set\n",
        "dev_chars, test_chars, dev_labels, test_labels = train_test_split(\n",
        "    temp_chars, temp_labels, test_size=2/3, random_state=42\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhij0-PiboIL"
      },
      "outputs": [],
      "source": [
        "# Step 1: 70% for the training set and 30% for the remaining data\n",
        "train_chars, temp_chars, train_labels, temp_labels = train_test_split(\n",
        "    characters, character_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: in the remaining data, 10% for the dev set and 20% for the test set\n",
        "dev_chars, test_chars, dev_labels, test_labels = train_test_split(\n",
        "    temp_chars, temp_labels, test_size=2/3, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n3vNPymZmx1"
      },
      "source": [
        "## Model defining"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- building the model of BERT-CRF\n",
        "\n",
        "Although CRF models are theoretically designed to reduce the label bias problem by globally normalizing sequence scores, this capability assumes sufficient data and complete training. The `PyTorch` CRF implementation is lightweight and does not inherently enforce transition constraints to guarantee valid label transitions, such as those required by BIO tagging schemes.\n",
        "\n",
        "Therefore, in practical structured prediction tasks like NER, it is necessary to manually define allowed transitions to ensure label consistency. In our model, we additionally define `_fix_bio_labels` to fix the label to avoid the problem.\n",
        "\n",
        "```\n",
        "class BertCRFTagger(nn.Module):\n",
        "    def __init__(self, bert_model_path: str, num_labels: int, label2id: dict):\n",
        "        super(BertCRFTagger, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_path)\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        self.classifier = nn.Linear(self.hidden_size, self.num_labels)\n",
        "        self.crf = CRF(self.num_labels, batch_first=True)\n",
        "\n",
        "        # label mapping\n",
        "        self.label2id = label2id\n",
        "        self.id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        emissions = self.classifier(sequence_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            mask = labels != -100\n",
        "            for i in range(mask.size(0)):\n",
        "                if not mask[i, 0]:\n",
        "                    for j in range(1, mask.size(1)):\n",
        "                        if mask[i, j]:\n",
        "                            labels[i, 0] = labels[i, j]\n",
        "                            mask[i, 0] = True\n",
        "                            break\n",
        "            labels = labels.clone()\n",
        "            labels[~mask] = 0\n",
        "            loss = -self.crf(emissions, labels, mask=mask, reduction='mean')\n",
        "            return loss\n",
        "        else:\n",
        "            prediction = self.crf.decode(emissions, mask=attention_mask.bool())\n",
        "            # fixing label bias\n",
        "            fixed_prediction = [self._fix_bio_labels(seq) for seq in prediction]\n",
        "            return fixed_prediction\n",
        "\n",
        "    def _fix_bio_labels(self, label_seq):\n",
        "        \"\"\"\n",
        "        fixing the case of incorresponding labels, such as ['B-BODY', 'I-CHEM']\n",
        "        \"\"\"\n",
        "        fixed = []\n",
        "        prev_type = None\n",
        "\n",
        "        for label_id in label_seq:\n",
        "            label = self.id2label[label_id]\n",
        "            if label == 'O':\n",
        "                fixed.append(label_id)\n",
        "                prev_type = None\n",
        "            elif label.startswith(\"B-\"):\n",
        "                fixed.append(label_id)\n",
        "                prev_type = label[2:]\n",
        "            elif label.startswith(\"I-\"):\n",
        "                current_type = label[2:]\n",
        "                if prev_type == current_type:\n",
        "                    fixed.append(label_id)\n",
        "                else:\n",
        "                    # starting with the B label\n",
        "                    new_label = \"B-\" + current_type\n",
        "                    fixed.append(self.label2id.get(new_label, label_id))\n",
        "                    prev_type = current_type\n",
        "            else:\n",
        "                fixed.append(label_id)\n",
        "                prev_type = None\n",
        "\n",
        "        return fixed\n",
        "```"
      ],
      "metadata": {
        "id": "OvIlziyIWH56"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNzsSc5uZmx1"
      },
      "outputs": [],
      "source": [
        "class BertCRFTagger(nn.Module):\n",
        "    def __init__(self, bert_model_path: str, num_labels: int, label2id: dict):\n",
        "        super(BertCRFTagger, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_path)\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        self.classifier = nn.Linear(self.hidden_size, self.num_labels)\n",
        "        self.crf = CRF(self.num_labels, batch_first=True)\n",
        "\n",
        "        # label mapping\n",
        "        self.label2id = label2id\n",
        "        self.id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        emissions = self.classifier(sequence_output)\n",
        "\n",
        "        if labels is not None:\n",
        "            mask = labels != -100\n",
        "            for i in range(mask.size(0)):\n",
        "                if not mask[i, 0]:\n",
        "                    for j in range(1, mask.size(1)):\n",
        "                        if mask[i, j]:\n",
        "                            labels[i, 0] = labels[i, j]\n",
        "                            mask[i, 0] = True\n",
        "                            break\n",
        "            labels = labels.clone()\n",
        "            labels[~mask] = 0\n",
        "            loss = -self.crf(emissions, labels, mask=mask, reduction='mean')\n",
        "            return loss\n",
        "        else:\n",
        "            prediction = self.crf.decode(emissions, mask=attention_mask.bool())\n",
        "            # fixing label bias\n",
        "            fixed_prediction = [self._fix_bio_labels(seq) for seq in prediction]\n",
        "            return fixed_prediction\n",
        "\n",
        "    def _fix_bio_labels(self, label_seq):\n",
        "        \"\"\"\n",
        "        fixing the case of incorresponding labels, such as ['B-BODY', 'I-CHEM']\n",
        "        \"\"\"\n",
        "        fixed = []\n",
        "        prev_type = None\n",
        "\n",
        "        for label_id in label_seq:\n",
        "            label = self.id2label[label_id]\n",
        "            if label == 'O':\n",
        "                fixed.append(label_id)\n",
        "                prev_type = None\n",
        "            elif label.startswith(\"B-\"):\n",
        "                fixed.append(label_id)\n",
        "                prev_type = label[2:]\n",
        "            elif label.startswith(\"I-\"):\n",
        "                current_type = label[2:]\n",
        "                if prev_type == current_type:\n",
        "                    fixed.append(label_id)\n",
        "                else:\n",
        "                    # starting with the B label\n",
        "                    new_label = \"B-\" + current_type\n",
        "                    fixed.append(self.label2id.get(new_label, label_id))\n",
        "                    prev_type = current_type\n",
        "            else:\n",
        "                fixed.append(label_id)\n",
        "                prev_type = None\n",
        "\n",
        "        return fixed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lMwEDVsZmxr"
      },
      "source": [
        "## Raw BERT + CRF\n",
        "\n",
        "In the following section, we utilise the pre-trained BERT which is **NOT** fine-tuned with some data in advance. It's noted that we need to use the corresponding tokeniser of the BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsAuV5JhZmxu"
      },
      "source": [
        "### text encoding\n",
        "It is important to conduct **tokenising** and **aligning** for text encoding before we work on BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhKmN0JAZmxu"
      },
      "source": [
        "#### tokenising\n",
        "\n",
        "`tokenizer` in HuggingFace Transformers is a built-in utility that handles not only tokenisation, but also:\n",
        "- Automatic padding: With `padding='max_length'`, it pads all sequences to the maximum length of the dataset.\n",
        "\n",
        "- Tensor output: With `return_tensors=\"pt\"`, it directly returns PyTorch tensors (no need to convert manually)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O3LUXkGZmxy"
      },
      "source": [
        "- encoding the texts\n",
        "\n",
        "**ðŸ’¡ Grouping shared arguments into a dictionary improves code neatness and readability.**\n",
        "\n",
        "```\n",
        "# grouping the shared arguments into a dictionary (packing)\n",
        "common_args = {\n",
        "    \"label2id\": label2id,\n",
        "    \"tokenizer_path\": \"bert-base-chinese\",\n",
        "    \"local_files_only\": False\n",
        "}\n",
        "\n",
        "# passing the shared arguments using dictionary unpacking (**common_args)\n",
        "train_input_ids_tensors, train_attention_mask_tensors, train_alignment_ids_list = character_encode(train_chars, train_labels, label2id, **common_args)\n",
        "dev_input_ids_tensors, dev_attention_mask_tensors, dev_alignment_ids_list = character_encode(dev_chars, dev_labels, label2id, **common_args)\n",
        "test_input_ids_tensors, test_attention_mask_tensors, test_alignment_ids_list = character_encode(test_chars, test_labels, label2id, **common_args)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etYMdX4LZmxy"
      },
      "outputs": [],
      "source": [
        "# grouping the shared arguments into a dictionary (packing)\n",
        "common_args = {\n",
        "    \"label2id\": label2id,\n",
        "    \"tokenizer_path\": \"bert-base-chinese\",\n",
        "    \"local_files_only\": False\n",
        "}\n",
        "\n",
        "# passing the shared arguments using dictionary unpacking (**common_args)\n",
        "train_input_ids_tensors, train_attention_mask_tensors, train_alignment_ids_list = character_encode(train_chars, train_labels, label2id, **common_args)\n",
        "dev_input_ids_tensors, dev_attention_mask_tensors, dev_alignment_ids_list = character_encode(dev_chars, dev_labels, label2id, **common_args)\n",
        "test_input_ids_tensors, test_attention_mask_tensors, test_alignment_ids_list = character_encode(test_chars, test_labels, label2id, **common_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5tJkmHPZmx0"
      },
      "source": [
        "- performing the alignment\n",
        "\n",
        "```\n",
        "train_label_tensors=align_labels(train_alignment_ids_list, train_labels, label2id)\n",
        "dev_label_tensors=align_labels(dev_alignment_ids_list, dev_labels, label2id)\n",
        "test_label_tensors=align_labels(test_alignment_ids_list, test_labels, label2id)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yAvtL7MZmx0"
      },
      "outputs": [],
      "source": [
        "train_label_tensors=align_labels(train_alignment_ids_list, train_labels, label2id)\n",
        "dev_label_tensors=align_labels(dev_alignment_ids_list, dev_labels, label2id)\n",
        "test_label_tensors=align_labels(test_alignment_ids_list, test_labels, label2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khkDjjjUZmx0"
      },
      "source": [
        "### `TensorDataset`\n",
        "\n",
        "We combine the processed input tensors and label tensors into `TensorDataset` objects, one for each data split:\n",
        "\n",
        "- **training set** â†’ `train_dataset`\n",
        "- **dev set** â†’ `dev_dataset`\n",
        "- **test set** â†’ `test_dataset`\n",
        "\n",
        "```\n",
        "train_dataset = TensorDataset(train_input_ids_tensors, train_attention_mask_tensors, train_label_tensors)\n",
        "dev_dataset = TensorDataset(dev_input_ids_tensors, dev_attention_mask_tensors, dev_label_tensors)\n",
        "test_dataset = TensorDataset(test_input_ids_tensors, test_attention_mask_tensors, test_label_tensors)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzB9OSyXZmx0"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(train_input_ids_tensors, train_attention_mask_tensors, train_label_tensors)\n",
        "dev_dataset = TensorDataset(dev_input_ids_tensors, dev_attention_mask_tensors, dev_label_tensors)\n",
        "test_dataset = TensorDataset(test_input_ids_tensors, test_attention_mask_tensors, test_label_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH6gPw5aZmx0"
      },
      "source": [
        "### `DataLoader`\n",
        "\n",
        "After creating `TensorDataset` objects for each data split, we wrap them with PyTorch `DataLoader` for efficient mini-batch loading during training and evaluation.\n",
        "\n",
        "We specify the `batch_size` and whether to shuffle the data (shuffling is used only for training).\n",
        "\n",
        "```\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader   = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8UmHYyLZmx0"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader   = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model training"
      ],
      "metadata": {
        "id": "5xQgOgNEbfj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- initialising the model\n",
        "\n",
        "```\n",
        "max_label_id = max(label2id.values())\n",
        "num_labels = max_label_id + 1\n",
        "raw_bert_model = BertCRFTagger(\n",
        "    bert_model_path=\"bert-base-chinese\",  # specifying the model: the model which is not fine-tuned in advance\n",
        "    num_labels=num_labels,\n",
        "    label2id=label2id\n",
        ")\n",
        "torch.manual_seed(24)\n",
        "```"
      ],
      "metadata": {
        "id": "NAVoPRQBblNr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NbgWj9rZmx2"
      },
      "outputs": [],
      "source": [
        "max_label_id = max(label2id.values())\n",
        "num_labels = max_label_id + 1\n",
        "raw_bert_model = BertCRFTagger(\n",
        "    bert_model_path=\"bert-base-chinese\",  # specifying the model: the model which is not fine-tuned in advance\n",
        "    num_labels=num_labels,\n",
        "    label2id=label2id\n",
        ")\n",
        "torch.manual_seed(24)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- training\n",
        "\n",
        "```\n",
        "train_losses = []\n",
        "dev_losses = []\n",
        "num_epochs = 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = AdamW(raw_bert_model.parameters(), lr=5e-5)\n",
        "raw_bert_model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
        "\n",
        "    # === Training ===\n",
        "    raw_bert_model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"[Epoch {epoch+1}] Training\", leave=False):\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = raw_bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # === Validation ===\n",
        "    raw_bert_model.eval()\n",
        "    dev_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dev_loader, desc=f\"[Epoch {epoch+1}] Validation\", leave=False):\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            loss = raw_bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            dev_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_dev_loss = dev_loss / len(dev_loader.dataset)\n",
        "    dev_losses.append(avg_dev_loss)\n",
        "\n",
        "    # === Summary ===\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} - Dev Loss: {avg_dev_loss:.4f}\")\n",
        "```"
      ],
      "metadata": {
        "id": "jpmkcBsqb9bE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcuB0pXhZmx2"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "train_losses = []\n",
        "dev_losses = []\n",
        "num_epochs = 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = AdamW(raw_bert_model.parameters(), lr=5e-5)\n",
        "raw_bert_model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
        "\n",
        "    # === Training ===\n",
        "    raw_bert_model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"[Epoch {epoch+1}] Training\", leave=False):\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = raw_bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # === Validation ===\n",
        "    raw_bert_model.eval()\n",
        "    dev_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dev_loader, desc=f\"[Epoch {epoch+1}] Validation\", leave=False):\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            loss = raw_bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            dev_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_dev_loss = dev_loss / len(dev_loader.dataset)\n",
        "    dev_losses.append(avg_dev_loss)\n",
        "\n",
        "    # === Summary ===\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} - Dev Loss: {avg_dev_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- plotting the loss curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "```\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(dev_losses, label='Dev Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "oT5hozK-cbCK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNWoKYKLyxwL"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(dev_losses, label='Dev Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### predicting\n",
        "\n",
        "```\n",
        "raw_bert_model.eval()\n",
        "\n",
        "pred_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # CRF prediction: list of predicted tag ids\n",
        "        predictions = raw_bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        for i in range(len(predictions)):\n",
        "            pred_seq = predictions[i]\n",
        "            gold_seq = labels[i]\n",
        "\n",
        "            cleaned_preds = []\n",
        "            cleaned_labels = []\n",
        "\n",
        "            for j, label_id in enumerate(gold_seq):\n",
        "                if label_id != -100:\n",
        "                    cleaned_preds.append(pred_seq[j])\n",
        "                    cleaned_labels.append(label_id.item())\n",
        "\n",
        "            pred_labels.append(cleaned_preds)\n",
        "            true_labels.append(cleaned_labels)\n",
        "```"
      ],
      "metadata": {
        "id": "U_vGtXvEclnL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZLDtpIcZmx2"
      },
      "outputs": [],
      "source": [
        "raw_bert_model.eval()\n",
        "\n",
        "pred_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # CRF prediction: list of predicted tag ids\n",
        "        predictions = raw_bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        for i in range(len(predictions)):\n",
        "            pred_seq = predictions[i]\n",
        "            gold_seq = labels[i]\n",
        "\n",
        "            cleaned_preds = []\n",
        "            cleaned_labels = []\n",
        "\n",
        "            for j, label_id in enumerate(gold_seq):\n",
        "                if label_id != -100:\n",
        "                    cleaned_preds.append(pred_seq[j])\n",
        "                    cleaned_labels.append(label_id.item())\n",
        "\n",
        "            pred_labels.append(cleaned_preds)\n",
        "            true_labels.append(cleaned_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluating\n",
        "\n",
        "```\n",
        "# flattening\n",
        "flattened_preds = [p for seq in pred_labels for p in seq]\n",
        "flattened_trues = [t for seq in true_labels for t in seq]\n",
        "\n",
        "# filtering out the O label\n",
        "interesting_labels = [k for k in label2id if k != \"O\"]\n",
        "interesting_ids = [label2id[k] for k in interesting_labels]\n",
        "\n",
        "# evaluating\n",
        "evaluation = classification_report(\n",
        "    flattened_trues,\n",
        "    flattened_preds,\n",
        "    labels=interesting_ids,\n",
        "    target_names=interesting_labels,\n",
        "    digits=4\n",
        ")\n",
        "print(evaluation)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "r0X_u2q5c2Vj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7UOHn7JZmx2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# flattening\n",
        "flattened_preds = [p for seq in pred_labels for p in seq]\n",
        "flattened_trues = [t for seq in true_labels for t in seq]\n",
        "\n",
        "# filtering out the O label\n",
        "interesting_labels = [k for k in label2id if k != \"O\"]\n",
        "interesting_ids = [label2id[k] for k in interesting_labels]\n",
        "\n",
        "# evaluating\n",
        "evaluation = classification_report(\n",
        "    flattened_trues,\n",
        "    flattened_preds,\n",
        "    labels=interesting_ids,\n",
        "    target_names=interesting_labels,\n",
        "    digits=4\n",
        ")\n",
        "print(evaluation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- inspecting the input, true labels, and the predicted labels"
      ],
      "metadata": {
        "id": "7jBc3Y3TdQoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwpe9PmsZmx2"
      },
      "outputs": [],
      "source": [
        "# converting id lists into label lists\n",
        "true_labels_str = [[id2label[idx] for idx in seq] for seq in true_labels]\n",
        "pred_labels_str = [[id2label[idx] for idx in seq] for seq in pred_labels]\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"Sentence\", test_chars[i])\n",
        "    print(\"True: \", true_labels_str[i])\n",
        "    print(\"Pred: \", pred_labels_str[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM7gPIfUZS6L"
      },
      "source": [
        "## Fine-tuned BERT + CRF\n",
        "\n",
        "In the following section, we utilise the pre-trained BERT which is fine-tuned with some data in advance. It's noted that we need to use the corresponding tokeniser of the BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfRLzT8It0M1"
      },
      "source": [
        "### text encoding\n",
        "It is important to conduct **tokenising** and **aligning** for text encoding before we work on BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDsqmI6xgyW0"
      },
      "source": [
        "#### tokenising\n",
        "\n",
        "`tokenizer` in HuggingFace Transformers is a built-in utility that handles not only tokenisation, but also:\n",
        "- Automatic padding: With `padding='max_length'`, it pads all sequences to the maximum length of the dataset.\n",
        "\n",
        "- Tensor output: With `return_tensors=\"pt\"`, it directly returns PyTorch tensors (no need to convert manually)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- mounting to Google Drive to access the fine-tuned model and tokeniser\n",
        "\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```"
      ],
      "metadata": {
        "id": "wA5flHXahbOJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZpoKmw3Ityv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0DbG5KirKWZ"
      },
      "source": [
        "- encoding the texts\n",
        "\n",
        "**ðŸ’¡ Grouping shared arguments into a dictionary improves code neatness and readability.**\n",
        "\n",
        "```\n",
        "# grouping the shared arguments into a dictionary (packing)\n",
        "common_args = {\n",
        "    \"label2id\": label2id,\n",
        "    \"tokenizer_path\": \"/content/drive/MyDrive/YOUR_PATH\",\n",
        "    \"local_files_only\": True\n",
        "}\n",
        "\n",
        "# passing the shared arguments using dictionary unpacking (**common_args)\n",
        "train_input_ids_tensors, train_attention_mask_tensors, train_alignment_ids_list = character_encode(train_chars, train_labels, **common_args)\n",
        "\n",
        "dev_input_ids_tensors, dev_attention_mask_tensors, dev_alignment_ids_list = character_encode(dev_chars, dev_labels, **common_args)\n",
        "\n",
        "test_input_ids_tensors, test_attention_mask_tensors, test_alignment_ids_list = character_encode(test_chars, test_labels, **common_args)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlj9faGEAyK1"
      },
      "outputs": [],
      "source": [
        "# grouping the shared arguments into a dictionary (packing)\n",
        "common_args = {\n",
        "    \"label2id\": label2id,\n",
        "    \"tokenizer_path\": \"/content/drive/MyDrive/YOUR_PATH\",\n",
        "    \"local_files_only\": True\n",
        "}\n",
        "\n",
        "# passing the shared arguments using dictionary unpacking (**common_args)\n",
        "train_input_ids_tensors, train_attention_mask_tensors, train_alignment_ids_list = character_encode(train_chars, train_labels, **common_args)\n",
        "\n",
        "dev_input_ids_tensors, dev_attention_mask_tensors, dev_alignment_ids_list = character_encode(dev_chars, dev_labels, **common_args)\n",
        "\n",
        "test_input_ids_tensors, test_attention_mask_tensors, test_alignment_ids_list = character_encode(test_chars, test_labels, **common_args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxS3qim0rxqy"
      },
      "source": [
        "- performing the alignment\n",
        "\n",
        "```\n",
        "train_label_tensors=align_labels(train_alignment_ids_list, train_labels, label2id)\n",
        "dev_label_tensors=align_labels(dev_alignment_ids_list, dev_labels, label2id)\n",
        "test_label_tensors=align_labels(test_alignment_ids_list, test_labels, label2id)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-qn2uSKeWfv"
      },
      "outputs": [],
      "source": [
        "train_label_tensors=align_labels(train_alignment_ids_list, train_labels, label2id)\n",
        "dev_label_tensors=align_labels(dev_alignment_ids_list, dev_labels, label2id)\n",
        "test_label_tensors=align_labels(test_alignment_ids_list, test_labels, label2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D36HY44ZeOH7"
      },
      "source": [
        "### `TensorDataset`\n",
        "\n",
        "We combine the processed input tensors and label tensors into `TensorDataset` objects, one for each data split:\n",
        "\n",
        "- **training set** â†’ `train_dataset`\n",
        "- **dev set** â†’ `dev_dataset`\n",
        "- **test set** â†’ `test_dataset`\n",
        "\n",
        "```\n",
        "train_dataset = TensorDataset(train_input_ids_tensors, train_attention_mask_tensors, train_label_tensors)\n",
        "dev_dataset = TensorDataset(dev_input_ids_tensors, dev_attention_mask_tensors, dev_label_tensors)\n",
        "test_dataset = TensorDataset(test_input_ids_tensors, test_attention_mask_tensors, test_label_tensors)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui8sM-ayeOH7"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(train_input_ids_tensors, train_attention_mask_tensors, train_label_tensors)\n",
        "dev_dataset = TensorDataset(dev_input_ids_tensors, dev_attention_mask_tensors, dev_label_tensors)\n",
        "test_dataset = TensorDataset(test_input_ids_tensors, test_attention_mask_tensors, test_label_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjtkfsFDeOH8"
      },
      "source": [
        "### `DataLoader`\n",
        "\n",
        "After creating `TensorDataset` objects for each data split, we wrap them with PyTorch `DataLoader` for efficient mini-batch loading during training and evaluation.\n",
        "\n",
        "We specify the `batch_size` and whether to shuffle the data (shuffling is used only for training).\n",
        "\n",
        "```\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader   = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ylCri2FeOH8"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dev_loader   = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model training"
      ],
      "metadata": {
        "id": "iMK8LVC_oVcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- initialising the model\n",
        "\n",
        "```\n",
        "max_label_id = max(label2id.values())\n",
        "num_labels = max_label_id + 1\n",
        "fine_tuned_bert_model = BertCRFTagger(\n",
        "    bert_model_path=\"/content/drive/MyDrive/NCCU/TA/1132_computational_linguistics/my_finetuned_bert/model\",  # specifying the model: the model which is not fine-tuned in advance\n",
        "    num_labels=num_labels,\n",
        "    label2id=label2id\n",
        ")\n",
        "torch.manual_seed(24)\n",
        "```"
      ],
      "metadata": {
        "id": "I5n1h6SAoVcr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBtLLo_moVcr"
      },
      "outputs": [],
      "source": [
        "max_label_id = max(label2id.values())\n",
        "num_labels = max_label_id + 1\n",
        "fine_tuned_bert_model = BertCRFTagger(\n",
        "    bert_model_path=\"/content/drive/MyDrive/NCCU/TA/1132_computational_linguistics/my_finetuned_bert/model\",  # specifying the model: the model which is not fine-tuned in advance\n",
        "    num_labels=num_labels,\n",
        "    label2id=label2id\n",
        ")\n",
        "torch.manual_seed(24)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- training\n",
        "\n",
        "```\n",
        "train_losses = []\n",
        "dev_losses = []\n",
        "num_epochs = 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = AdamW(fine_tuned_bert_model.parameters(), lr=5e-5)\n",
        "fine_tuned_bert_model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
        "\n",
        "    # === Training ===\n",
        "    fine_tuned_bert_model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"[Epoch {epoch+1}] Training\", leave=False):\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = fine_tuned_bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # === Validation ===\n",
        "    fine_tuned_bert_model.eval()\n",
        "    dev_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dev_loader, desc=f\"[Epoch {epoch+1}] Validation\", leave=False):\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            loss = fine_tuned_bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            dev_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_dev_loss = dev_loss / len(dev_loader.dataset)\n",
        "    dev_losses.append(avg_dev_loss)\n",
        "\n",
        "    # === Summary ===\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} - Dev Loss: {avg_dev_loss:.4f}\")\n",
        "```"
      ],
      "metadata": {
        "id": "-NLAjLSCoVcs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPuCFtfooVct"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "dev_losses = []\n",
        "num_epochs = 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = AdamW(fine_tuned_bert_model.parameters(), lr=5e-5)\n",
        "fine_tuned_bert_model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
        "\n",
        "    # === Training ===\n",
        "    fine_tuned_bert_model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"[Epoch {epoch+1}] Training\", leave=False):\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = fine_tuned_bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # === Validation ===\n",
        "    fine_tuned_bert_model.eval()\n",
        "    dev_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dev_loader, desc=f\"[Epoch {epoch+1}] Validation\", leave=False):\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            loss = fine_tuned_bert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            dev_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_dev_loss = dev_loss / len(dev_loader.dataset)\n",
        "    dev_losses.append(avg_dev_loss)\n",
        "\n",
        "    # === Summary ===\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} - Dev Loss: {avg_dev_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- plotting the loss curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "```\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(dev_losses, label='Dev Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "ub72DsV6oVct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlplrvwEoVcu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(dev_losses, label='Dev Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### predicting\n",
        "\n",
        "```\n",
        "fine_tuned_bert_model.eval()\n",
        "\n",
        "pred_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # CRF prediction: list of predicted tag ids\n",
        "        predictions = fine_tuned_bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        for i in range(len(predictions)):\n",
        "            pred_seq = predictions[i]\n",
        "            gold_seq = labels[i]\n",
        "\n",
        "            cleaned_preds = []\n",
        "            cleaned_labels = []\n",
        "\n",
        "            for j, label_id in enumerate(gold_seq):\n",
        "                if label_id != -100:\n",
        "                    cleaned_preds.append(pred_seq[j])\n",
        "                    cleaned_labels.append(label_id.item())\n",
        "\n",
        "            pred_labels.append(cleaned_preds)\n",
        "            true_labels.append(cleaned_labels)\n",
        "```"
      ],
      "metadata": {
        "id": "6nrhZKxyoVcu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_RUuNY8oVcu"
      },
      "outputs": [],
      "source": [
        "fine_tuned_bert_model.eval()\n",
        "\n",
        "pred_labels = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # CRF prediction: list of predicted tag ids\n",
        "        predictions = fine_tuned_bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        for i in range(len(predictions)):\n",
        "            pred_seq = predictions[i]\n",
        "            gold_seq = labels[i]\n",
        "\n",
        "            cleaned_preds = []\n",
        "            cleaned_labels = []\n",
        "\n",
        "            for j, label_id in enumerate(gold_seq):\n",
        "                if label_id != -100:\n",
        "                    cleaned_preds.append(pred_seq[j])\n",
        "                    cleaned_labels.append(label_id.item())\n",
        "\n",
        "            pred_labels.append(cleaned_preds)\n",
        "            true_labels.append(cleaned_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluating\n",
        "\n",
        "```\n",
        "# flattening\n",
        "flattened_preds = [p for seq in pred_labels for p in seq]\n",
        "flattened_trues = [t for seq in true_labels for t in seq]\n",
        "\n",
        "# filtering out the O label\n",
        "interesting_labels = [k for k in label2id if k != \"O\"]\n",
        "interesting_ids = [label2id[k] for k in interesting_labels]\n",
        "\n",
        "# evaluating\n",
        "evaluation = classification_report(\n",
        "    flattened_trues,\n",
        "    flattened_preds,\n",
        "    labels=interesting_ids,\n",
        "    target_names=interesting_labels,\n",
        "    digits=4\n",
        ")\n",
        "print(evaluation)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "VgH5SqvfoVcu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eHq-Zr_oVcv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# flattening\n",
        "flattened_preds = [p for seq in pred_labels for p in seq]\n",
        "flattened_trues = [t for seq in true_labels for t in seq]\n",
        "\n",
        "# filtering out the O label\n",
        "interesting_labels = [k for k in label2id if k != \"O\"]\n",
        "interesting_ids = [label2id[k] for k in interesting_labels]\n",
        "\n",
        "# evaluating\n",
        "evaluation = classification_report(\n",
        "    flattened_trues,\n",
        "    flattened_preds,\n",
        "    labels=interesting_ids,\n",
        "    target_names=interesting_labels,\n",
        "    digits=4\n",
        ")\n",
        "print(evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- inspecting the input, true labels, and the predicted labels\n",
        "\n",
        "```\n",
        "# converting id lists into label lists\n",
        "true_labels_str = [[id2label[idx] for idx in seq] for seq in true_labels]\n",
        "pred_labels_str = [[id2label[idx] for idx in seq] for seq in pred_labels]\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"Sentence\", test_chars[i])\n",
        "    print(\"True: \", true_labels_str[i])\n",
        "    print(\"Pred: \", pred_labels_str[i])\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "092FVM11oVcv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoLoaRCKoVcv"
      },
      "outputs": [],
      "source": [
        "# converting id lists into label lists\n",
        "true_labels_str = [[id2label[idx] for idx in seq] for seq in true_labels]\n",
        "pred_labels_str = [[id2label[idx] for idx in seq] for seq in pred_labels]\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"Sentence\", test_chars[i])\n",
        "    print(\"True: \", true_labels_str[i])\n",
        "    print(\"Pred: \", pred_labels_str[i])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}